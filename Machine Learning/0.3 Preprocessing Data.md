### Importing Libraries {#introduction}
```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

### Importing Data Set
```python
dataset = pd.read_csv('Data.csv')
```

#### Dividing the data in features and dependent variables
1. **Features** - The columns which you use to predict
2. **Dependent variables** - The column you want to predict

- Dataset is imported as a dataframe.
- Dataframe is collection of rows

**To dividing data set into features and dependent variables we use** `dataset.iloc[row, col]`
`dataset.iloc[row slicing, col slicing]`, locating index of dataset
example : `dataset.iloc[1:4, 3:5]`, 1-3 rows and 3-4 cols are included
or we can just select one row / column using
`dataset.iloc[4, 5]` (only 4th row and 5th column)

After selecting the part of data we want we can convert that into numpy array using `values` attribute
```python
x = dataset.iloc[:, :-1].values # all rows and columns except last column
y = dataset.iloc[:, -1].values # all rows and only last column
```

Here `x` is features and `y` is dependent variables in form of numpy array

### Handling missing data
We can handle by missing data by
1. Deleting the particular row (applicable for large data sets)
2. Replacing the missing value with average or median or mode of other values in the column

But generally we use `mean`
To fill the missing data we use `sklearn` library

Importing the `SimpleImputer` class
```python
from sklearn.impute import SimpleImputer
```

creating an object of the class,
`missing_values` the type of values are going to replace, since we are using numpy arrays, we put `np.nan`
`strategy` is the what value we are going to replace with, here we are using `mean`
```python
impute = SimpleImputer(missing_values=np.nan, strategy='mean')
```

Replacing the missing values with the mean
`fit` method actually makes the calculations and stores what to replace
`transform` method actually replaces the data
for both these methods we pass parameters as the rows or columns we need to look in the data
```python
impute.fit(x[:, 1:3])
x[:, 1:3] = impute.transform(x[:, 1:3])
```

### Encoding Categorial Data
#### Encoding Independent Variables
- We need to encode text data
- But we can't just put it as 0, 1, 2
- If we do that, there might be some unwanted correlation in the data, which is not needed
- Thus we encode it to binary 1s and 0s, so that there will be no correlation
- This encoding is called **One Hot Encoding**

For this we need to import 
```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
```

Initialization
```python
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')
x = ct.fit_transform(x)
```

1. **`ColumnTransformer`**:
   `ColumnTransformer` is a scikit-learn class used for applying transformations to specific columns of a dataset. It allows for different transformations to be applied to different columns.

2. **`transformers`**:
   This is a list of tuples specifying the transformers to be applied to specific columns. Each tuple contains three elements:
   - **`'encoder'`**: A string identifier for the transformer (in this case, it's called "encoder").
   - **`OneHotEncoder()`**: The transformer to be applied, in this case, `OneHotEncoder()`, which is used for one-hot encoding categorical features.
   - **`[0]`**: The indices of the columns to which the transformer will be applied. In this case, it's column 0.

3. **`remainder='passthrough'`**:
   The `remainder` parameter specifies what to do with the remaining columns that were not specified in the `transformers`. 
   - **`'passthrough'`**: This option means that any columns not transformed should be left untouched and passed through to the output.

In summary, this `ColumnTransformer` is set up to apply the `OneHotEncoder` transformation to the first column (`[0]`) of the dataset and leave the rest of the columns untouched (`'passthrough'`). The result will be a dataset where the first column is one-hot encoded, and the rest of the columns remain as they were.

#### Encoding Dependent Variable
- Dependent is generally *yes* or *no*, which will be encoded to *1* and *0* respectively
- So we use **Label Encoder**

For that we need to import
```python
from sklearn.preprocessing import LabelEncoder
```

Initialization
```python
le = LabelEncoder()
y = le.fit_transform(y)
```

### Splitting the data in train and test sets

For splitting data we need to import
```python
from sklearn.model_selection import train_test_split
```

Initialization
```python
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)
```

1. This line calls the `train_test_split` function to split the data into training and testing sets.
    
    - `x` is typically the feature matrix or the input data.
    - `y` is typically the target variable or the output labels associated with the features.
    - `test_size=0.3` specifies that 30% of the data will be used for testing, and the remaining 70% will be used for training.
    - `random_state=1` sets a specific seed for the random number generator, ensuring that the data split is reproducible. Using the same seed will yield the same data split each time, which is useful for reproducibility in machine learning experiments.

The function returns four arrays:

- `x_train`: Contains the features for the training set.
- `x_test`: Contains the features for the testing set.
- `y_train`: Contains the labels/targets for the training set.
- `y_test`: Contains the labels/targets for the testing set.

These arrays are commonly used to train a machine learning model on the training data (`x_train`, `y_train`) and then evaluate its performance on the unseen testing data (`x_test`, `y_test`).


> [!Important] Splitting -> Feature scaling
> **Feature Scaling** is done after splitting data in train and testing, because feature scaling is done before splitting then test data might leak into train data, generally feature scaling involves selecting minimum values of entire data or finding mean of the entire data set, so this might make it so that test data is partly affecting training data

### Feature Scaling
- Scaling is not mandatory for all models

**Which Scaler to use**
- We have two options, either **standard** scaler or **normal** scaler
- **Normalization** works better our data is distributed normally. So it doesn't work well every case
- **Standardization** works wells is all cases

To apply standardization we need to import
```python
from sklearn.preprocessing import StandardScaler
```

Initialization
```python
ss = StandardScalar()
x_train[:, 3:] = ss.fit_transform(x[:, 3:])
x_test[:, 3:] = ss.transform(x[:, 3:])
```

**We won't scale encoded data**
- We scale data to put the data in limits, for standardization the data falls between (-3, 3), the encoded data we have is already in that range, thus we won't scale that
- Moreover the encoded data has a certain pattern to it, (in OneHotEncoder the data is in form of binary bits), so if we scale it, that pattern would be lost.

**Should we fit and transform our test data?**
We have scaled the training data and we are going to use that to train our model, thus our test data should also be scaled exactly as our train data, thus we **only transform** our test data.