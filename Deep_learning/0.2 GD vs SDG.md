Gradient Descent (GD) and Stochastic Gradient Descent (SGD) are optimization algorithms commonly used in training machine learning models. Here's a brief overview of each:

1. **Gradient Descent (GD):**
   - **Batch Processing:** In vanilla Gradient Descent, the entire training dataset is used to compute the gradient of the cost function with respect to the model parameters (weights).
   - **Update Rule:** The model parameters are updated once per iteration by subtracting a fraction (learning rate) of the computed gradient.
   - **Computational Cost:** Can be computationally expensive, especially for large datasets, as it involves computing gradients for the entire dataset in each iteration.
   - **Smooth Convergence:** Generally converges smoothly to the global minimum.

2. **Stochastic Gradient Descent (SGD):**
   - **Online Processing:** In SGD, only one training example (or a small batch of examples) is used to compute the gradient at each iteration.
   - **Update Rule:** Model parameters are updated more frequently, and the learning rate can be adaptively adjusted.
   - **Computational Cost:** Less computationally expensive compared to GD, especially for large datasets. It can be faster since it processes one example at a time.
   - **Noisy Convergence:** The updates can introduce more noise, leading to a more erratic convergence path. However, this noise can sometimes help escape local minima.
   - **Use Cases:** Particularly useful for large datasets and in scenarios where the cost function has many local minima.

3. **Mini-Batch Gradient Descent:**
   - A compromise between GD and SGD, where a small batch of training examples (typically 32, 64, or 128) is used to compute the gradient.
   - This approach combines some advantages of both GD and SGD, offering a balance between computational efficiency and convergence smoothness.

**Choosing Between GD and SGD:**
   - GD is generally used when the dataset fits in memory, and smooth convergence is desired.
   - SGD is often preferred for large datasets, especially in deep learning, where computing the gradient for the entire dataset can be impractical.
   - Mini-batch GD is commonly used in practice as it provides a good balance between computational efficiency and convergence characteristics.

The choice between GD and SGD depends on factors such as dataset size, computational resources, and the specific characteristics of the optimization problem at hand.