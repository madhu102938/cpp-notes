Normalization and activation functions serve different purposes in the context of neural networks.

1. **Normalization:**
   - Normalization involves scaling the input features to ensure that they have a similar scale. Common normalization techniques include z-score normalization (subtracting mean and dividing by standard deviation) or min-max scaling (scaling features to a specific range).
   - The goal of normalization is to stabilize and accelerate the training process, making it easier for the optimizer to find the optimal weights during the learning process.
   - Normalization is typically applied before the data is fed into the neural network and is often done as part of the data preprocessing.

2. **Activation Function (in the context of the input layer):**
   - Activation functions introduce non-linearities to the network, allowing it to learn complex relationships and patterns in the data.
   - Activation functions are usually applied to hidden layers and the output layer. Common activation functions include 'relu' (Rectified Linear Unit), 'sigmoid', and 'tanh'.
   - Applying an activation function to the input layer is not a common practice. The input layer is usually treated as a pass-through layer, and non-linearities are introduced in subsequent hidden layers.

In summary, normalization and activation functions have distinct purposes. Normalization is about scaling input features for numerical stability and faster convergence during training. Activation functions are applied to introduce non-linearities, allowing the neural network to learn complex mappings between inputs and outputs. They are typically applied to hidden layers, not the input layer.

- Basically neural networks without activation functions are basically linear equations, as linear equations cannot predict everything, we need to introduce non-linearity
- To introduce non-linearity, we use activation functions