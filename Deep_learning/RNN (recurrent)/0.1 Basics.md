![why not standard](./images/why_not_standard.png)

![forward prop](./images/forward_prop.png)

![simplified rnn](./images/simplified_rnn.png)

![back prop through time](./images/back_prop_rnn.png)

## Different RNN architectures
1. many to many
2. many to one
3. one to many

![different rnn architecture](diff_rnn_arch.png)

## Vanishing gradients with RNNs
with longer sequences, output at time stamp `<tx>` generally depends on some few time steps just before that, instead of depending on all the time steps before it.

## GRU (Gated Recurrent Unit)



